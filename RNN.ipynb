{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naNoH7ThwiPk"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "ledgfyzZwP9i"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEvyehtQxKAA"
      },
      "source": [
        "# Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "_cell_guid": "55dd8ffd-6011-49a3-a1fe-c6933c4187b7",
        "_uuid": "840f7b1c60d1a2d5b2222a7c53b2b9d08aac9169",
        "id": "0wnoGD1-wP9o",
        "outputId": "267b8d02-1688-4a40-e512-8d81aa616159",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# load data\n",
        "train = pd.read_csv(r\"train.csv\",dtype = np.float32)\n",
        "\n",
        "# split data into features(pixels) and labels(numbers from 0 to 9)\n",
        "targets_numpy = train.label.values\n",
        "features_numpy = train.loc[:,train.columns != \"label\"].values/255 # normalization\n",
        "\n",
        "# train test split. Size of train data is 80% and size of test data is 20%. \n",
        "features_train, features_test, targets_train, targets_test = train_test_split(features_numpy,\n",
        "                                                                             targets_numpy,\n",
        "                                                                             test_size = 0.2,\n",
        "                                                                             random_state = 42) \n",
        "\n",
        "# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
        "featuresTrain = torch.from_numpy(features_train)\n",
        "targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n",
        "\n",
        "# create feature and targets tensor for test set.\n",
        "featuresTest = torch.from_numpy(features_test)\n",
        "targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long\n",
        "\n",
        "# batch_size, epoch and iteration\n",
        "batch_size = 100\n",
        "n_iters = 10000\n",
        "num_epochs = n_iters / (len(features_train) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "# Pytorch train and test sets\n",
        "train = TensorDataset(featuresTrain,targetsTrain)\n",
        "test = TensorDataset(featuresTest,targetsTest)\n",
        "\n",
        "# data loader\n",
        "train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n",
        "test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "# visualize one of the images in data set\n",
        "plt.imshow(features_numpy[10].reshape(28,28))\n",
        "plt.axis(\"off\")\n",
        "plt.title(str(targets_numpy[10]))\n",
        "plt.savefig('graph.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR6cTleVxRRq"
      },
      "source": [
        "# Create RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "_cell_guid": "7fbe419e-7ce2-4d72-bb31-8b27e8161f1b",
        "_uuid": "bb1b6d4fb5504400ed7678d8e95d0a4478b5f409",
        "id": "N0SOd-QfwP9q"
      },
      "outputs": [],
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
        "        super(RNNModel, self).__init__()\n",
        "        \n",
        "        # Number of hidden dimensions\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # Number of hidden layers\n",
        "        self.layer_dim = layer_dim\n",
        "        \n",
        "        # RNN\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
        "        \n",
        "        # Readout layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # Initialize hidden state with zeros\n",
        "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
        "            \n",
        "        # One time step\n",
        "        out, hn = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :]) \n",
        "        return out\n",
        "\n",
        "# batch_size, epoch and iteration\n",
        "batch_size = 100\n",
        "n_iters = 8000\n",
        "num_epochs = n_iters / (len(features_train) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "# Pytorch train and test sets\n",
        "train = TensorDataset(featuresTrain,targetsTrain)\n",
        "test = TensorDataset(featuresTest,targetsTest)\n",
        "\n",
        "# data loader\n",
        "train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n",
        "test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n",
        "    \n",
        "# Create RNN\n",
        "input_dim = 28    # input dimension\n",
        "hidden_dim = 100  # hidden layer dimension\n",
        "layer_dim = 1     # number of hidden layers\n",
        "output_dim = 10   # output dimension\n",
        "\n",
        "model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = nn.CrossEntropyLoss()\n",
        "\n",
        "# SGD Optimizer\n",
        "learning_rate = 0.05\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwoCLzfpxYKj"
      },
      "source": [
        "# Train RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "32786a5c-0388-412d-b6da-ee5ace604eda",
        "_uuid": "9c935ac4a1d1964b85513da422ebf60085dca0e3",
        "id": "PFh_MLSTwP9s",
        "outputId": "38e7f672-b2be-4814-ada0-f5ccf73a1aa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500  Loss: nan  Accuracy: 8.720930099487305 %\n",
            "Iteration: 1000  Loss: nan  Accuracy: 8.720930099487305 %\n",
            "Iteration: 1500  Loss: nan  Accuracy: 8.720930099487305 %\n",
            "Iteration: 2000  Loss: nan  Accuracy: 8.720930099487305 %\n",
            "Iteration: 2500  Loss: nan  Accuracy: 8.720930099487305 %\n",
            "Iteration: 3000  Loss: nan  Accuracy: 8.720930099487305 %\n"
          ]
        }
      ],
      "source": [
        "seq_dim = 28  \n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "count = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        train  = Variable(images.view(-1, seq_dim, input_dim))\n",
        "        labels = Variable(labels )\n",
        "            \n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward propagation\n",
        "        outputs = model(train)\n",
        "        \n",
        "        # Calculate softmax and ross entropy loss\n",
        "        loss = error(outputs, labels)\n",
        "        \n",
        "        # Calculating gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        count += 1\n",
        "        \n",
        "        if count % 250 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                images = Variable(images.view(-1, seq_dim, input_dim))\n",
        "                \n",
        "                # Forward propagation\n",
        "                outputs = model(images)\n",
        "                \n",
        "                # Get predictions from the maximum value\n",
        "                predicted = torch.max(outputs.data, 1)[1]\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "                \n",
        "                correct += (predicted == labels).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / float(total)\n",
        "            \n",
        "            # store loss and iteration\n",
        "            loss_list.append(loss.data)\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "            if count % 500 == 0:\n",
        "                # Print Loss\n",
        "                print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbfQgdDnxjRe"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0e527a85-b600-4e40-a0ef-850537db2ab1",
        "_uuid": "0cb7130ea6e22093d6d5cb1284822b0b76b8d66c",
        "id": "52E3ZoZdwP9u"
      },
      "outputs": [],
      "source": [
        "# visualization loss \n",
        "plt.plot(iteration_list,loss_list)\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"RNN: Loss vs Number of iteration\")\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"RNN: Accuracy vs Number of iteration\")\n",
        "plt.savefig('graph.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy4j9Usv0aD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}